# ETL - Airflow

Мой первый проект по ETL с использованием Apache Airflow. В проекте реализуется ETL процес по сбору, транисформации и загрузки данных в бд Postgres. Цель проекта попрактиковаться в изучении data engineering.

## Описание и структура

`etl_kolesa_dag.py` — основной файл DAG с задачами extract, transform, load.

1. **Extract** - Парсит страницы сайта kolesa.kz, извлекает из каждого объявления данные.
2. **Transform** - Очищает и преобразует данные.
3. **Load** - Загружает преобразованные данные в таблицу `toyota_file` бд Postgres.

---

## Настройка и запуск

1. После сохранения проекта запустите команду `docker-compose up -d` в корне проекта.
2. Проверьте работоспособность всех контейнеров командой `docker ps`.
3. Зайдите в Airflow webserver по адресу `localhost:8080` и запустите DAG.
4. DAG настроен на ежедневный запуск в 22:00, можно запустить вручную.
5. После выполнения dag зайдите в pgAdmin по адресу `localhost:8081` и подключитесь к серверу postgres по данным в .env. Там же вы можете найти загруженный toyota_file в таблицах.
---

